# Max AI Agent Configuration
# Copy this file to .env and fill in your settings

# ── LLM Provider Settings ──────────────────────────────────────────────────
# LLM_PROVIDER options: "local" (Ollama), "cloud" (OpenRouter), "auto" (try local first)
LLM_PROVIDER=auto

# ── OpenRouter (Cloud LLM) ─────────────────────────────────────────────────
# Get free API key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-v1-your-key-here
# Model options: google/gemini-2.0-flash-exp:free, meta-llama/llama-3-8b-instruct:free, etc.
OPENROUTER_MODEL=google/gemini-2.0-flash-exp:free

# ── Ollama (Local LLM) ─────────────────────────────────────────────────────
# Ollama server URL (default is local)
OLLAMA_BASE_URL=http://localhost:11434
# Model: phi3:mini, llama2, mistral, etc. (must be installed with: ollama pull <model>)
OLLAMA_MODEL=phi3:mini
# Context window size for Ollama
OLLAMA_NUM_CTX=2048

# ── Chrome Browser ─────────────────────────────────────────────────────────
# Path to Chrome executable
CHROME_PATH=C:\Program Files\Google\Chrome\Application\chrome.exe
# Port for Chrome remote debugging (for automation)
CHROME_DEBUG_PORT=9222
# Profile directory for Chrome debugging session
CHROME_USER_DATA=C:\chrome-debug-profile

# ── Speech Recognition (Whisper) ───────────────────────────────────────────
# Model size: tiny, base, small, medium, large-v3
# Larger = better quality but slower; tiny = fast but less accurate
WHISPER_MODEL=small
# Device: cuda (GPU), cpu (slower)
WHISPER_DEVICE=cuda

# ── Text-to-Speech (TTS) ───────────────────────────────────────────────────
# Voice gender: male, female
TTS_VOICE_GENDER=male

# ── Wake Word ──────────────────────────────────────────────────────────────
# Word that triggers listening (e.g., "max", "jarvis", "alexa")
WAKE_WORD=max

# ── Logging ────────────────────────────────────────────────────────────────
# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
